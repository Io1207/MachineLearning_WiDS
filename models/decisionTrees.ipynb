{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode():\n",
    "    def __init__(self, data, feature_idx, feature_val, prediction_probs, information_gain) -> None:\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.prediction_probs = prediction_probs\n",
    "        self.information_gain = information_gain\n",
    "        self.feature_importance = self.data.shape[0] * self.information_gain\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def node_def(self):\n",
    "        if (self.left or self.right):\n",
    "            return f\"NODE | Information Gain = {self.information_gain} | Split IF X[{self.feature_idx}] < {self.feature_val} THEN left O/W right\"\n",
    "        else:\n",
    "            unique_values, value_counts = np.unique(self.data[:,-1], return_counts=True)\n",
    "            output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)])            \n",
    "            return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1,min_information_gain=0.0, numb_of_features_splitting=None,amount_of_say=None):   \n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.numb_of_features_splitting = numb_of_features_splitting\n",
    "        self.amount_of_say = amount_of_say\n",
    "\n",
    "    def _entropy(self, class_probabilities: list):\n",
    "        return sum([-p * np.log2(p) for p in class_probabilities if p>0])\n",
    "    \n",
    "    def _class_probabilities(self, labels: list):\n",
    "        total_count = len(labels)\n",
    "        return [label_count / total_count for label_count in Counter(labels).values()]\n",
    "\n",
    "    def _data_entropy(self, labels: list):\n",
    "        return self._entropy(self._class_probabilities(labels))\n",
    "    \n",
    "    def _partition_entropy(self, subsets: list):\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
    "    \n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        \n",
    "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "\n",
    "        return group1, group2\n",
    "    \n",
    "    def _select_features_to_use(self, data: np.array):\n",
    "        feature_idx = list(range(data.shape[1]-1))\n",
    "\n",
    "        if self.numb_of_features_splitting == \"sqrt\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx))))\n",
    "        elif self.numb_of_features_splitting == \"log\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx))))\n",
    "        else:\n",
    "            feature_idx_to_use = feature_idx\n",
    "\n",
    "        return feature_idx_to_use\n",
    "        \n",
    "    def _find_best_split(self, data: np.array):\n",
    "        min_part_entropy = 1e9\n",
    "        feature_idx_to_use =  self._select_features_to_use(data)\n",
    "\n",
    "        for idx in feature_idx_to_use:\n",
    "            feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25))\n",
    "            for feature_val in feature_vals:\n",
    "                g1, g2, = self._split(data, idx, feature_val)\n",
    "                part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]])\n",
    "                if part_entropy < min_part_entropy:\n",
    "                    min_part_entropy = part_entropy\n",
    "                    min_entropy_feature_idx = idx\n",
    "                    min_entropy_feature_val = feature_val\n",
    "                    g1_min, g2_min = g1, g2\n",
    "\n",
    "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
    "\n",
    "    def _find_label_probs(self, data: np.array) -> np.array:\n",
    "\n",
    "        labels_as_integers = data[:,-1].astype(int)\n",
    "        total_labels = len(labels_as_integers)\n",
    "        label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "        for i, label in enumerate(self.labels_in_train):\n",
    "            label_index = np.where(labels_as_integers == i)[0]\n",
    "            if len(label_index) > 0:\n",
    "                label_probabilities[i] = len(label_index) / total_labels\n",
    "\n",
    "        return label_probabilities\n",
    "\n",
    "    def _create_tree(self, data: np.array, current_depth: int):\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self._find_best_split(data)\n",
    "        label_probabilities = self._find_label_probs(data)\n",
    "        node_entropy = self._entropy(label_probabilities)\n",
    "        information_gain = node_entropy - split_entropy\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain)\n",
    "\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
    "            return node\n",
    "        elif information_gain < self.min_information_gain:\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self._create_tree(split_1_data, current_depth)\n",
    "        node.right = self._create_tree(split_2_data, current_depth)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
    "        \n",
    "        node = self.tree\n",
    "        while node:\n",
    "            pred_probs = node.prediction_probs\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return pred_probs\n",
    "\n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        \n",
    "        self.labels_in_train = np.unique(Y_train)\n",
    "        train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1)\n",
    "\n",
    "        self.tree = self._create_tree(data=train_data, current_depth=0)\n",
    "\n",
    "\n",
    "        self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0)\n",
    "        self._calculate_feature_importance(self.tree)\n",
    "\n",
    "        self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()}\n",
    "\n",
    "    def predict_proba(self, X_set: np.array) -> np.array:\n",
    "\n",
    "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
    "        \n",
    "        return pred_probs\n",
    "\n",
    "    def predict(self, X_set: np.array) -> np.array:\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        \n",
    "        return preds    \n",
    "        \n",
    "    def _print_recursive(self, node: TreeNode, level=0) -> None:\n",
    "        if node != None:\n",
    "            self._print_recursive(node.left, level + 1)\n",
    "            print('    ' * 4 * level + '-> ' + node.node_def())\n",
    "            self._print_recursive(node.right, level + 1)\n",
    "\n",
    "    def print_tree(self) -> None:\n",
    "        self._print_recursive(node=self.tree)\n",
    "\n",
    "    def _calculate_feature_importance(self, node):\n",
    "        if node != None:\n",
    "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
    "            self._calculate_feature_importance(node.left)\n",
    "            self._calculate_feature_importance(node.right)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
